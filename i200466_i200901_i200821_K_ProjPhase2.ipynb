{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKwaK6SIwXP3",
        "outputId": "c0825422-8c50-400d-f252-5af9b5936a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph from /content/Dataset.txt...\n",
            "Graph loaded in 26.85 seconds with 4847571 nodes and 7970363 edges.\n",
            "Using delta value: 1\n",
            "Running initial SSSP...\n",
            "Initial SSSP completed in 8.62 seconds.\n",
            "Sample distances from source node:\n",
            "Node 0: 0\n",
            "Node 1: 1\n",
            "Node 2: 1\n",
            "Node 3: 1\n",
            "Node 4: 1\n",
            "Node 5: 1\n",
            "Node 6: 1\n",
            "Node 7: 1\n",
            "Node 8: 1\n",
            "Node 9: 1\n",
            "\n",
            "Applying 500 updates...\n",
            "Updates applied in 122.19 seconds.\n",
            "Identified 1254 affected nodes.\n",
            "Running incremental SSSP...\n",
            "Incremental SSSP completed in 1.92 seconds.\n",
            "Speedup factor (initial / incremental): 4.49x\n",
            "Updated sample distances:\n",
            "Node 0: 0\n",
            "Node 1: 1\n",
            "Node 2: 1\n",
            "Node 3: 1\n",
            "Node 4: 1\n",
            "Node 5: 1\n",
            "Node 6: 1\n",
            "Node 7: 1\n",
            "Node 8: 1\n",
            "Node 9: 1\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import random\n",
        "import time\n",
        "import enum\n",
        "import threading\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Global mutex for thread synchronization\n",
        "global_mutex = threading.Lock()\n",
        "\n",
        "@dataclass\n",
        "class Edge:\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.num_nodes: int = 0\n",
        "        self.edges: List[Edge] = []\n",
        "        self.adj: Dict[int, List[Tuple[int, int]]] = {}\n",
        "        self.node_to_partition: List[int] = []\n",
        "\n",
        "    def compute_adjacency_list(self):\n",
        "        self.adj.clear()\n",
        "        for e in self.edges:\n",
        "            if e.src not in self.adj:\n",
        "                self.adj[e.src] = []\n",
        "            self.adj[e.src].append((e.dest, e.weight))\n",
        "\n",
        "    def identify_boundary_nodes(self, my_partition: int) -> Set[int]:\n",
        "        boundary = set()\n",
        "        for node, neighbors in self.adj.items():\n",
        "            if self.node_to_partition[node] != my_partition:\n",
        "                continue\n",
        "            for neighbor, _ in neighbors:\n",
        "                if neighbor < len(self.node_to_partition) and self.node_to_partition[neighbor] != my_partition:\n",
        "                    boundary.add(node)\n",
        "                    break\n",
        "        return boundary\n",
        "\n",
        "    def load_from_file(self, filename: str):\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                if line.startswith('#'):\n",
        "                    continue\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 2:\n",
        "                    src = int(parts[0])\n",
        "                    dest = int(parts[1])\n",
        "                    self.edges.append(Edge(src, dest))\n",
        "\n",
        "        all_nodes = set(e.src for e in self.edges).union(e.dest for e in self.edges)\n",
        "        self.num_nodes = max(all_nodes) + 1 if all_nodes else 0\n",
        "        self.node_to_partition = [i % 4 for i in range(self.num_nodes)]\n",
        "        self.compute_adjacency_list()\n",
        "\n",
        "class UpdateType(enum.Enum):\n",
        "    INSERT = 1\n",
        "    DELETE = 2\n",
        "\n",
        "@dataclass\n",
        "class EdgeUpdate:\n",
        "    type: UpdateType\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "def process_updates_thread(graph: Graph, updates: List[EdgeUpdate], start_idx: int, end_idx: int, adjacency_changed: List[bool]):\n",
        "    local_changed = False\n",
        "    for i in range(start_idx, end_idx):\n",
        "        upd = updates[i]\n",
        "        if upd.src >= graph.num_nodes or upd.dest >= graph.num_nodes:\n",
        "            with global_mutex:\n",
        "                print(f\"Warning: Ignoring invalid update {upd.src}->{upd.dest}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        with global_mutex:\n",
        "            if upd.type == UpdateType.INSERT:\n",
        "                exists = any(neigh == upd.dest for neigh, _ in graph.adj.get(upd.src, []))\n",
        "                if not exists:\n",
        "                    graph.edges.append(Edge(upd.src, upd.dest, upd.weight))\n",
        "                    local_changed = True\n",
        "            elif upd.type == UpdateType.DELETE:\n",
        "                original_len = len(graph.adj.get(upd.src, []))\n",
        "                graph.edges = [e for e in graph.edges if not (e.src == upd.src and e.dest == upd.dest)]\n",
        "                if upd.src in graph.adj:\n",
        "                    graph.adj[upd.src] = [(d, w) for d, w in graph.adj[upd.src] if d != upd.dest]\n",
        "                    if len(graph.adj[upd.src]) < original_len:\n",
        "                        local_changed = True\n",
        "\n",
        "    if local_changed:\n",
        "        with global_mutex:\n",
        "            adjacency_changed[0] = True\n",
        "\n",
        "def apply_updates(graph: Graph, updates: List[EdgeUpdate]):\n",
        "    adjacency_changed = [False]\n",
        "    num_threads = max(1, min(4, len(updates))) if len(updates) > 100 else 1  # Adjusting thread count based on updates\n",
        "    chunk_size = (len(updates) + num_threads - 1) // num_threads\n",
        "    threads = []\n",
        "\n",
        "    for i in range(num_threads):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min(len(updates), (i + 1) * chunk_size)\n",
        "        if start_idx >= len(updates):\n",
        "            break\n",
        "        t = threading.Thread(target=process_updates_thread, args=(graph, updates, start_idx, end_idx, adjacency_changed))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    if adjacency_changed[0]:\n",
        "        graph.compute_adjacency_list()\n",
        "\n",
        "def compute_optimal_delta(graph: Graph) -> int:\n",
        "    if not graph.edges:\n",
        "        return 1\n",
        "    weights = [e.weight for e in graph.edges]\n",
        "    avg_weight = sum(weights) / len(weights)\n",
        "    return max(1, min(int(avg_weight), 100))  # Using average instead of median\n",
        "\n",
        "def identify_affected_nodes(graph: Graph, updates: List[EdgeUpdate], current_dist: List[float]) -> Set[int]:\n",
        "    affected = set()\n",
        "    INF = float('inf')\n",
        "    for upd in updates:\n",
        "        affected.update([upd.src, upd.dest])\n",
        "        if upd.type == UpdateType.INSERT:\n",
        "            if current_dist[upd.src] != INF and current_dist[upd.src] + upd.weight < current_dist[upd.dest]:\n",
        "                affected.add(upd.dest)\n",
        "        elif upd.type == UpdateType.DELETE:\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.src, []))\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.dest, []))\n",
        "    return affected\n",
        "\n",
        "def process_bucket_nodes(graph: Graph, dist: List[float], nodes: Set[int], delta: int, next_buckets: Dict[int, Set[int]], process_light: bool):\n",
        "    INF = float('inf')\n",
        "    local_relaxed = {}\n",
        "\n",
        "    for u in nodes:\n",
        "        for v, weight in graph.adj.get(u, []):\n",
        "            if (process_light and weight <= delta) or (not process_light and weight > delta):\n",
        "                new_dist = dist[u] + weight\n",
        "                if new_dist < dist[v]:\n",
        "                    local_relaxed[v] = min(local_relaxed.get(v, INF), new_dist)\n",
        "\n",
        "    with global_mutex:\n",
        "        for v, new_dist in local_relaxed.items():\n",
        "            if new_dist < dist[v]:\n",
        "                dist[v] = new_dist\n",
        "                bucket = int(new_dist // delta)\n",
        "                if bucket not in next_buckets:\n",
        "                    next_buckets[bucket] = set()\n",
        "                next_buckets[bucket].add(v)\n",
        "\n",
        "def delta_stepping_sssp(graph: Graph, dist: List[float], source: int, delta: int, affected_nodes: Optional[Set[int]] = None):\n",
        "    INF = float('inf')\n",
        "    if affected_nodes is None or not affected_nodes:\n",
        "        dist[:] = [INF] * len(dist)\n",
        "        dist[source] = 0\n",
        "        affected_nodes = {source}\n",
        "\n",
        "    max_bucket = 1000000\n",
        "    buckets = [set() for _ in range(max_bucket)]\n",
        "    for node in affected_nodes:\n",
        "        if node < len(dist) and dist[node] != INF:\n",
        "            b_idx = int(dist[node] // delta)\n",
        "            buckets[b_idx].add(node)\n",
        "\n",
        "    num_threads = max(1, min(4, len(affected_nodes)))  # Dynamic thread count based on affected nodes\n",
        "\n",
        "    for i in range(max_bucket):\n",
        "        while buckets[i]:\n",
        "            current_nodes = buckets[i]\n",
        "            buckets[i] = set()\n",
        "\n",
        "            next_light, next_heavy = {}, {}\n",
        "\n",
        "            node_chunks = [set() for _ in range(num_threads)]\n",
        "            for idx, node in enumerate(current_nodes):\n",
        "                node_chunks[idx % num_threads].add(node)\n",
        "\n",
        "            # Light edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(graph, dist, chunk, delta, next_light, True))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_light.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "            # Heavy edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(graph, dist, chunk, delta, next_heavy, False))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_heavy.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "def main():\n",
        "    graph = Graph()\n",
        "    dataset_file = \"/content/Dataset.txt\"  # Change path as needed\n",
        "\n",
        "    print(f\"Loading graph from {dataset_file}...\")\n",
        "    start_time = time.time()\n",
        "    graph.load_from_file(dataset_file)\n",
        "    load_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Graph loaded in {load_time:.2f} seconds with {graph.num_nodes} nodes and {len(graph.edges)} edges.\")\n",
        "\n",
        "    source_node = 0\n",
        "    distances = [float('inf')] * graph.num_nodes\n",
        "\n",
        "    delta = compute_optimal_delta(graph)\n",
        "    print(f\"Using delta value: {delta}\")\n",
        "\n",
        "    print(\"Running initial SSSP...\")\n",
        "    start_time = time.time()\n",
        "    delta_stepping_sssp(graph, distances, source_node, delta)\n",
        "    sssp_time = time.time() - start_time\n",
        "    print(f\"Initial SSSP completed in {sssp_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"Sample distances from source node:\")\n",
        "    for i in range(min(10, graph.num_nodes)):\n",
        "        dist_str = \"INF\" if distances[i] == float('inf') else str(distances[i])\n",
        "        print(f\"Node {i}: {dist_str}\")\n",
        "\n",
        "    num_updates = 500\n",
        "    updates = []\n",
        "    random.seed(42)\n",
        "    for _ in range(num_updates):\n",
        "        upd_type = UpdateType.INSERT if random.random() < 0.5 else UpdateType.DELETE\n",
        "        src = random.randint(0, graph.num_nodes - 1)\n",
        "        dest = random.randint(0, graph.num_nodes - 1)\n",
        "        updates.append(EdgeUpdate(upd_type, src, dest))\n",
        "\n",
        "    print(f\"\\nApplying {num_updates} updates...\")\n",
        "    start_time = time.time()\n",
        "    apply_updates(graph, updates)\n",
        "    update_time = time.time() - start_time\n",
        "    print(f\"Updates applied in {update_time:.2f} seconds.\")\n",
        "\n",
        "    affected_nodes = identify_affected_nodes(graph, updates, distances)\n",
        "    print(f\"Identified {len(affected_nodes)} affected nodes.\")\n",
        "\n",
        "    print(\"Running incremental SSSP...\")\n",
        "    start_time = time.time()\n",
        "    delta_stepping_sssp(graph, distances, source_node, delta, affected_nodes)\n",
        "    incremental_time = time.time() - start_time\n",
        "    print(f\"Incremental SSSP completed in {incremental_time:.2f} seconds.\")\n",
        "    print(f\"Speedup factor (initial / incremental): {sssp_time / incremental_time:.2f}x\")\n",
        "\n",
        "    print(\"Updated sample distances:\")\n",
        "    for i in range(min(10, graph.num_nodes)):\n",
        "        dist_str = \"INF\" if distances[i] == float('inf') else str(distances[i])\n",
        "        print(f\"Node {i}: {dist_str}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import time\n",
        "import enum\n",
        "import threading\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# Global mutex for thread synchronization\n",
        "global_mutex = threading.Lock()\n",
        "\n",
        "@dataclass\n",
        "class Edge:\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.num_nodes: int = 0\n",
        "        self.edges: List[Edge] = []\n",
        "        self.adj: Dict[int, List[Tuple[int, int]]] = {}\n",
        "        self.node_to_partition: List[int] = []\n",
        "\n",
        "    def compute_adjacency_list(self):\n",
        "        self.adj.clear()\n",
        "        for e in self.edges:\n",
        "            if e.src not in self.adj:\n",
        "                self.adj[e.src] = []\n",
        "            self.adj[e.src].append((e.dest, e.weight))\n",
        "\n",
        "    def identify_boundary_nodes(self, my_partition: int) -> Set[int]:\n",
        "        boundary = set()\n",
        "        for node, neighbors in self.adj.items():\n",
        "            if self.node_to_partition[node] != my_partition:\n",
        "                continue\n",
        "            for neighbor, _ in neighbors:\n",
        "                if neighbor < len(self.node_to_partition) and self.node_to_partition[neighbor] != my_partition:\n",
        "                    boundary.add(node)\n",
        "                    break\n",
        "        return boundary\n",
        "\n",
        "    def load_from_file(self, filename: str):\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                if line.startswith('#'):\n",
        "                    continue\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 2:\n",
        "                    src = int(parts[0])\n",
        "                    dest = int(parts[1])\n",
        "                    self.edges.append(Edge(src, dest))\n",
        "\n",
        "        all_nodes = set(e.src for e in self.edges).union(e.dest for e in self.edges)\n",
        "        self.num_nodes = max(all_nodes) + 1 if all_nodes else 0\n",
        "        self.node_to_partition = [i % 4 for i in range(self.num_nodes)]\n",
        "        self.compute_adjacency_list()\n",
        "\n",
        "class UpdateType(enum.Enum):\n",
        "    INSERT = 1\n",
        "    DELETE = 2\n",
        "\n",
        "@dataclass\n",
        "class EdgeUpdate:\n",
        "    type: UpdateType\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "def process_updates_thread(graph: Graph, updates: List[EdgeUpdate], start_idx: int, end_idx: int, adjacency_changed: List[bool]):\n",
        "    local_changed = False\n",
        "    for i in range(start_idx, end_idx):\n",
        "        upd = updates[i]\n",
        "        if upd.src >= graph.num_nodes or upd.dest >= graph.num_nodes:\n",
        "            with global_mutex:\n",
        "                print(f\"Warning: Ignoring invalid update {upd.src}->{upd.dest}\", file=sys.stderr)\n",
        "            continue\n",
        "\n",
        "        with global_mutex:\n",
        "            if upd.type == UpdateType.INSERT:\n",
        "                exists = any(neigh == upd.dest for neigh, _ in graph.adj.get(upd.src, []))\n",
        "                if not exists:\n",
        "                    graph.edges.append(Edge(upd.src, upd.dest, upd.weight))\n",
        "                    local_changed = True\n",
        "            elif upd.type == UpdateType.DELETE:\n",
        "                original_len = len(graph.adj.get(upd.src, []))\n",
        "                graph.edges = [e for e in graph.edges if not (e.src == upd.src and e.dest == upd.dest)]\n",
        "                if upd.src in graph.adj:\n",
        "                    graph.adj[upd.src] = [(d, w) for d, w in graph.adj[upd.src] if d != upd.dest]\n",
        "                    if len(graph.adj[upd.src]) < original_len:\n",
        "                        local_changed = True\n",
        "\n",
        "    if local_changed:\n",
        "        with global_mutex:\n",
        "            adjacency_changed[0] = True\n",
        "\n",
        "def apply_updates(graph: Graph, updates: List[EdgeUpdate]):\n",
        "    adjacency_changed = [False]\n",
        "    num_threads = max(1, min(4, len(updates)))\n",
        "    chunk_size = (len(updates) + num_threads - 1) // num_threads\n",
        "    threads = []\n",
        "\n",
        "    for i in range(num_threads):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min(len(updates), (i + 1) * chunk_size)\n",
        "        if start_idx >= len(updates):\n",
        "            break\n",
        "        t = threading.Thread(target=process_updates_thread, args=(graph, updates, start_idx, end_idx, adjacency_changed))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    if adjacency_changed[0]:\n",
        "        graph.compute_adjacency_list()\n",
        "\n",
        "def compute_optimal_delta(graph: Graph) -> int:\n",
        "    if not graph.edges:\n",
        "        return 1\n",
        "    weights = sorted(e.weight for e in graph.edges)\n",
        "    median = weights[len(weights) // 2]\n",
        "    return max(1, min(median, 100))\n",
        "\n",
        "def identify_affected_nodes(graph: Graph, updates: List[EdgeUpdate], current_dist: List[float]) -> Set[int]:\n",
        "    affected = set()\n",
        "    INF = float('inf')\n",
        "    for upd in updates:\n",
        "        affected.update([upd.src, upd.dest])\n",
        "        if upd.type == UpdateType.INSERT:\n",
        "            if current_dist[upd.src] != INF and current_dist[upd.src] + upd.weight < current_dist[upd.dest]:\n",
        "                affected.add(upd.dest)\n",
        "        elif upd.type == UpdateType.DELETE:\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.src, []))\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.dest, []))\n",
        "    return affected\n",
        "\n",
        "def process_bucket_nodes(graph: Graph, dist: List[float], nodes: Set[int], delta: int, next_buckets: Dict[int, Set[int]], process_light: bool):\n",
        "    INF = float('inf')\n",
        "    local_relaxed = {}\n",
        "\n",
        "    for u in nodes:\n",
        "        for v, weight in graph.adj.get(u, []):\n",
        "            if (process_light and weight <= delta) or (not process_light and weight > delta):\n",
        "                new_dist = dist[u] + weight\n",
        "                if new_dist < dist[v]:\n",
        "                    local_relaxed[v] = min(local_relaxed.get(v, INF), new_dist)\n",
        "\n",
        "    with global_mutex:\n",
        "        for v, new_dist in local_relaxed.items():\n",
        "            if new_dist < dist[v]:\n",
        "                dist[v] = new_dist\n",
        "                bucket = int(new_dist // delta)\n",
        "                if bucket not in next_buckets:\n",
        "                    next_buckets[bucket] = set()\n",
        "                next_buckets[bucket].add(v)\n",
        "\n",
        "def delta_stepping_sssp(graph: Graph, dist: List[float], source: int, delta: int, affected_nodes: Optional[Set[int]] = None):\n",
        "    INF = float('inf')\n",
        "    if affected_nodes is None or not affected_nodes:\n",
        "        dist[:] = [INF] * len(dist)\n",
        "        dist[source] = 0\n",
        "        affected_nodes = {source}\n",
        "\n",
        "    max_bucket = 1000000\n",
        "    buckets = [set() for _ in range(max_bucket)]\n",
        "    for node in affected_nodes:\n",
        "        if node < len(dist) and dist[node] != INF:\n",
        "            b_idx = int(dist[node] // delta)\n",
        "            buckets[b_idx].add(node)\n",
        "\n",
        "    num_threads = max(1, min(4, len(buckets)))\n",
        "\n",
        "    for i in range(max_bucket):\n",
        "        while buckets[i]:\n",
        "            current_nodes = buckets[i]\n",
        "            buckets[i] = set()\n",
        "\n",
        "            next_light, next_heavy = {}, {}\n",
        "\n",
        "            node_chunks = [set() for _ in range(num_threads)]\n",
        "            for idx, node in enumerate(current_nodes):\n",
        "                node_chunks[idx % num_threads].add(node)\n",
        "\n",
        "            # Light edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(graph, dist, chunk, delta, next_light, True))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_light.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "            # Heavy edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(graph, dist, chunk, delta, next_heavy, False))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_heavy.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "def main():\n",
        "    graph = Graph()\n",
        "    dataset_file = \"Dataset.txt\"  # Change path as needed\n",
        "\n",
        "    print(f\"Loading graph from {dataset_file}...\")\n",
        "    start_time = time.time()\n",
        "    graph.load_from_file(dataset_file)\n",
        "    load_time = time.time() - start_time\n",
        "\n",
        "    print(f\"Graph loaded in {load_time:.2f} seconds with {graph.num_nodes} nodes and {len(graph.edges)} edges.\")\n",
        "\n",
        "    source_node = 0\n",
        "    distances = [float('inf')] * graph.num_nodes\n",
        "\n",
        "    delta = compute_optimal_delta(graph)\n",
        "    print(f\"Using delta value: {delta}\")\n",
        "\n",
        "    print(\"Running initial SSSP...\")\n",
        "    start_time = time.time()\n",
        "    delta_stepping_sssp(graph, distances, source_node, delta)\n",
        "    sssp_time = time.time() - start_time\n",
        "    print(f\"Initial SSSP completed in {sssp_time:.2f} seconds.\")\n",
        "\n",
        "    print(\"Sample distances from source node:\")\n",
        "    for i in range(min(10, graph.num_nodes)):\n",
        "        dist_str = \"INF\" if distances[i] == float('inf') else str(distances[i])\n",
        "        print(f\"Node {i}: {dist_str}\")\n",
        "\n",
        "    num_updates = 100\n",
        "    updates = []\n",
        "    random.seed(42)\n",
        "    for _ in range(num_updates):\n",
        "        upd_type = UpdateType.INSERT if random.random() < 0.5 else UpdateType.DELETE\n",
        "        src = random.randint(0, graph.num_nodes - 1)\n",
        "        dest = random.randint(0, graph.num_nodes - 1)\n",
        "        updates.append(EdgeUpdate(upd_type, src, dest))\n",
        "\n",
        "    print(f\"\\nApplying {num_updates} updates...\")\n",
        "    start_time = time.time()\n",
        "    apply_updates(graph, updates)\n",
        "    update_time = time.time() - start_time\n",
        "    print(f\"Updates applied in {update_time:.2f} seconds.\")\n",
        "\n",
        "    affected_nodes = identify_affected_nodes(graph, updates, distances)\n",
        "    print(f\"Identified {len(affected_nodes)} affected nodes.\")\n",
        "\n",
        "    print(\"Running incremental SSSP...\")\n",
        "    start_time = time.time()\n",
        "    delta_stepping_sssp(graph, distances, source_node, delta, affected_nodes)\n",
        "    incremental_time = time.time() - start_time\n",
        "    print(f\"Incremental SSSP completed in {incremental_time:.2f} seconds.\")\n",
        "    print(f\"Speedup factor (initial / incremental): {sssp_time / incremental_time:.2f}x\")\n",
        "\n",
        "    print(\"Updated sample distances:\")\n",
        "    for i in range(min(10, graph.num_nodes)):\n",
        "        dist_str = \"INF\" if distances[i] == float('inf') else str(distances[i])\n",
        "        print(f\"Node {i}: {dist_str}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghYXHfF80D5L",
        "outputId": "2531ed3b-8210-4e0c-9fb4-25403a3771b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading graph from Dataset.txt...\n",
            "Graph loaded in 0.00 seconds with 50 nodes and 277 edges.\n",
            "Using delta value: 1\n",
            "Running initial SSSP...\n",
            "Initial SSSP completed in 1.87 seconds.\n",
            "Sample distances from source node:\n",
            "Node 0: 0\n",
            "Node 1: 1\n",
            "Node 2: 1\n",
            "Node 3: 1\n",
            "Node 4: 1\n",
            "Node 5: 1\n",
            "Node 6: 1\n",
            "Node 7: 1\n",
            "Node 8: 1\n",
            "Node 9: 1\n",
            "\n",
            "Applying 100 updates...\n",
            "Updates applied in 0.01 seconds.\n",
            "Identified 50 affected nodes.\n",
            "Running incremental SSSP...\n",
            "Incremental SSSP completed in 2.07 seconds.\n",
            "Speedup factor (initial / incremental): 0.91x\n",
            "Updated sample distances:\n",
            "Node 0: 0\n",
            "Node 1: 1\n",
            "Node 2: 1\n",
            "Node 3: 1\n",
            "Node 4: 1\n",
            "Node 5: 1\n",
            "Node 6: 1\n",
            "Node 7: 1\n",
            "Node 8: 1\n",
            "Node 9: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install matplotlib memory_profiler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi4o7UVTVXyj",
        "outputId": "57db1156-6aee-47af-d946-e196a30baed0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory_profiler) (5.9.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory_profiler\n",
            "Successfully installed memory_profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import time\n",
        "import enum\n",
        "import threading\n",
        "import logging\n",
        "import cProfile\n",
        "import pstats\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import matplotlib.pyplot as plt\n",
        "import heapq\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    format='%(asctime)s - %(threadName)s - %(message)s')\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Global mutex for thread synchronization\n",
        "global_mutex = threading.Lock()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Edge:\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "\n",
        "class Graph:\n",
        "    def __init__(self):\n",
        "        self.num_nodes: int = 0\n",
        "        self.edges: List[Edge] = []\n",
        "        self.adj: Dict[int, List[Tuple[int, int]]] = {}\n",
        "        self.node_to_partition: List[int] = []\n",
        "\n",
        "    def compute_adjacency_list(self):\n",
        "        self.adj.clear()\n",
        "        for e in self.edges:\n",
        "            if e.src not in self.adj:\n",
        "                self.adj[e.src] = []\n",
        "            self.adj[e.src].append((e.dest, e.weight))\n",
        "\n",
        "    def identify_boundary_nodes(self, my_partition: int) -> Set[int]:\n",
        "        boundary = set()\n",
        "        for node, neighbors in self.adj.items():\n",
        "            if self.node_to_partition[node] != my_partition:\n",
        "                continue\n",
        "            for neighbor, _ in neighbors:\n",
        "                if neighbor < len(self.node_to_partition) and self.node_to_partition[neighbor] != my_partition:\n",
        "                    boundary.add(node)\n",
        "                    break\n",
        "        return boundary\n",
        "\n",
        "    def load_from_file(self, filename: str):\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                if line.startswith('#'):\n",
        "                    continue\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 2:\n",
        "                    src = int(parts[0])\n",
        "                    dest = int(parts[1])\n",
        "                    self.edges.append(Edge(src, dest))\n",
        "\n",
        "        all_nodes = set(e.src for e in self.edges).union(\n",
        "            e.dest for e in self.edges)\n",
        "        self.num_nodes = max(all_nodes) + 1 if all_nodes else 0\n",
        "        self.node_to_partition = [i % 4 for i in range(self.num_nodes)]\n",
        "        self.compute_adjacency_list()\n",
        "\n",
        "\n",
        "class UpdateType(enum.Enum):\n",
        "    INSERT = 1\n",
        "    DELETE = 2\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class EdgeUpdate:\n",
        "    type: UpdateType\n",
        "    src: int\n",
        "    dest: int\n",
        "    weight: int = 1\n",
        "\n",
        "\n",
        "def process_updates_thread(graph: Graph, updates: List[EdgeUpdate], start_idx: int, end_idx: int, adjacency_changed: List[bool]):\n",
        "    local_changed = False\n",
        "    for i in range(start_idx, end_idx):\n",
        "        upd = updates[i]\n",
        "        if upd.src >= graph.num_nodes or upd.dest >= graph.num_nodes:\n",
        "            with global_mutex:\n",
        "                logger.warning(\n",
        "                    f\"Ignoring invalid update {upd.src}->{upd.dest}\")\n",
        "            continue\n",
        "\n",
        "        with global_mutex:\n",
        "            if upd.type == UpdateType.INSERT:\n",
        "                exists = any(neigh == upd.dest for neigh,\n",
        "                             _ in graph.adj.get(upd.src, []))\n",
        "                if not exists:\n",
        "                    graph.edges.append(Edge(upd.src, upd.dest, upd.weight))\n",
        "                    local_changed = True\n",
        "            elif upd.type == UpdateType.DELETE:\n",
        "                original_len = len(graph.adj.get(upd.src, []))\n",
        "                graph.edges = [e for e in graph.edges if not (\n",
        "                    e.src == upd.src and e.dest == upd.dest)]\n",
        "                if upd.src in graph.adj:\n",
        "                    graph.adj[upd.src] = [\n",
        "                        (d, w) for d, w in graph.adj[upd.src] if d != upd.dest]\n",
        "                    if len(graph.adj[upd.src]) < original_len:\n",
        "                        local_changed = True\n",
        "\n",
        "    if local_changed:\n",
        "        with global_mutex:\n",
        "            adjacency_changed[0] = True\n",
        "\n",
        "\n",
        "def apply_updates(graph: Graph, updates: List[EdgeUpdate]):\n",
        "    adjacency_changed = [False]\n",
        "    num_threads = max(1, min(4, len(updates))) if len(updates) > 100 else 1\n",
        "    chunk_size = (len(updates) + num_threads - 1) // num_threads\n",
        "    threads = []\n",
        "\n",
        "    for i in range(num_threads):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min(len(updates), (i + 1) * chunk_size)\n",
        "        if start_idx >= len(updates):\n",
        "            break\n",
        "        t = threading.Thread(target=process_updates_thread, args=(\n",
        "            graph, updates, start_idx, end_idx, adjacency_changed))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    if adjacency_changed[0]:\n",
        "        graph.compute_adjacency_list()\n",
        "\n",
        "\n",
        "def compute_optimal_delta(graph: Graph) -> int:\n",
        "    if not graph.edges:\n",
        "        return 1\n",
        "    weights = [e.weight for e in graph.edges]\n",
        "    avg_weight = sum(weights) / len(weights)\n",
        "    return max(1, min(int(avg_weight), 100))\n",
        "\n",
        "\n",
        "def identify_affected_nodes(graph: Graph, updates: List[EdgeUpdate], current_dist: List[float]) -> Set[int]:\n",
        "    affected = set()\n",
        "    INF = float('inf')\n",
        "    for upd in updates:\n",
        "        affected.update([upd.src, upd.dest])\n",
        "        if upd.type == UpdateType.INSERT:\n",
        "            if current_dist[upd.src] != INF and current_dist[upd.src] + upd.weight < current_dist[upd.dest]:\n",
        "                affected.add(upd.dest)\n",
        "        elif upd.type == UpdateType.DELETE:\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.src, []))\n",
        "            affected.update(n for n, _ in graph.adj.get(upd.dest, []))\n",
        "    return affected\n",
        "\n",
        "\n",
        "def process_bucket_nodes(graph: Graph, dist: List[float], nodes: Set[int], delta: int, next_buckets: Dict[int, Set[int]], process_light: bool):\n",
        "    INF = float('inf')\n",
        "    local_relaxed = {}\n",
        "\n",
        "    for u in nodes:\n",
        "        for v, weight in graph.adj.get(u, []):\n",
        "            if (process_light and weight <= delta) or (not process_light and weight > delta):\n",
        "                new_dist = dist[u] + weight\n",
        "                if new_dist < dist[v]:\n",
        "                    local_relaxed[v] = min(local_relaxed.get(v, INF), new_dist)\n",
        "\n",
        "    with global_mutex:\n",
        "        for v, new_dist in local_relaxed.items():\n",
        "            if new_dist < dist[v]:\n",
        "                dist[v] = new_dist\n",
        "                bucket = int(new_dist // delta)\n",
        "                if bucket not in next_buckets:\n",
        "                    next_buckets[bucket] = set()\n",
        "                next_buckets[bucket].add(v)\n",
        "\n",
        "\n",
        "def delta_stepping_sssp(graph: Graph, dist: List[float], source: int, delta: int, affected_nodes: Optional[Set[int]] = None):\n",
        "    INF = float('inf')\n",
        "    if affected_nodes is None or not affected_nodes:\n",
        "        dist[:] = [INF] * len(dist)\n",
        "        dist[source] = 0\n",
        "        affected_nodes = {source}\n",
        "\n",
        "    max_bucket = 1000000\n",
        "    buckets = [set() for _ in range(max_bucket)]\n",
        "    for node in affected_nodes:\n",
        "        if node < len(dist) and dist[node] != INF:\n",
        "            b_idx = int(dist[node] // delta)\n",
        "            buckets[b_idx].add(node)\n",
        "\n",
        "    num_threads = max(1, min(4, len(affected_nodes)))\n",
        "\n",
        "    for i in range(max_bucket):\n",
        "        while buckets[i]:\n",
        "            current_nodes = buckets[i]\n",
        "            buckets[i] = set()\n",
        "\n",
        "            next_light, next_heavy = {}, {}\n",
        "\n",
        "            node_chunks = [set() for _ in range(num_threads)]\n",
        "            for idx, node in enumerate(current_nodes):\n",
        "                node_chunks[idx % num_threads].add(node)\n",
        "\n",
        "            # Light edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(\n",
        "                        graph, dist, chunk, delta, next_light, True))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_light.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "            # Heavy edges\n",
        "            threads = []\n",
        "            for chunk in node_chunks:\n",
        "                if chunk:\n",
        "                    t = threading.Thread(target=process_bucket_nodes, args=(\n",
        "                        graph, dist, chunk, delta, next_heavy, False))\n",
        "                    threads.append(t)\n",
        "                    t.start()\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            for b_idx, nodes in next_heavy.items():\n",
        "                if b_idx < len(buckets):\n",
        "                    buckets[b_idx].update(nodes)\n",
        "\n",
        "\n",
        "def dijkstra_sssp(graph: Graph, source: int) -> List[float]:\n",
        "    INF = float('inf')\n",
        "    dist = [INF] * graph.num_nodes\n",
        "    dist[source] = 0\n",
        "    heap = [(0, source)]\n",
        "    visited = set()\n",
        "\n",
        "    while heap:\n",
        "        current_dist, u = heapq.heappop(heap)\n",
        "        if u in visited:\n",
        "            continue\n",
        "        visited.add(u)\n",
        "        for v, weight in graph.adj.get(u, []):\n",
        "            if dist[v] > current_dist + weight:\n",
        "                dist[v] = current_dist + weight\n",
        "                heapq.heappush(heap, (dist[v], v))\n",
        "    return dist\n",
        "\n",
        "\n",
        "def benchmark_sssp(graph: Graph, source: int, delta: int):\n",
        "    # Delta-Stepping\n",
        "    dist_delta = [float('inf')] * graph.num_nodes\n",
        "    start_time = time.time()\n",
        "    delta_stepping_sssp(graph, dist_delta, source, delta)\n",
        "    delta_time = time.time() - start_time\n",
        "\n",
        "    # Dijkstra's\n",
        "    start_time = time.time()\n",
        "    dist_dijkstra = dijkstra_sssp(graph, source)\n",
        "    dijkstra_time = time.time() - start_time\n",
        "\n",
        "    # Validate correctness\n",
        "    assert all(abs(a - b) < 1e-6 for a, b in zip(dist_delta,\n",
        "               dist_dijkstra)), \"Results mismatch!\"\n",
        "\n",
        "    return delta_time, dijkstra_time\n",
        "\n",
        "\n",
        "def profile_memory(func, *args, **kwargs):\n",
        "    \"\"\"Dummy memory profiler when memory_profiler package is not available\"\"\"\n",
        "    # Run the function once to get execution time\n",
        "    start_time = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Return dummy memory value (0) and log timing info\n",
        "    logger.info(\n",
        "        f\"Function {func.__name__} executed in {elapsed:.2f}s (memory profiling disabled)\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def plot_results(thread_counts, times, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(thread_counts, times, marker='o', linestyle='-', color='b')\n",
        "    plt.xlabel(\"Thread Count\")\n",
        "    plt.ylabel(\"Execution Time (s)\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{title.replace(' ', '_')}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    graph = Graph()\n",
        "    # Update path as needed\n",
        "    dataset_file = \"/content/Dataset.txt\"\n",
        "\n",
        "    logger.info(f\"Loading graph from {dataset_file}...\")\n",
        "    start_time = time.time()\n",
        "    graph.load_from_file(dataset_file)\n",
        "    load_time = time.time() - start_time\n",
        "    logger.info(\n",
        "        f\"Graph loaded in {load_time:.2f} seconds with {graph.num_nodes} nodes and {len(graph.edges)} edges.\")\n",
        "\n",
        "    source_node = 0\n",
        "    delta = compute_optimal_delta(graph)\n",
        "    logger.info(f\"Optimal delta: {delta}\")\n",
        "\n",
        "    # Benchmark SSSP algorithms\n",
        "    delta_time, dijkstra_time = benchmark_sssp(graph, source_node, delta)\n",
        "    logger.info(\n",
        "        f\"Delta-Stepping Time: {delta_time:.2f}s | Dijkstra Time: {dijkstra_time:.2f}s | Speedup: {dijkstra_time / delta_time:.2f}x\")\n",
        "\n",
        "    # Memory profiling (disabled but kept for structure)\n",
        "    mem_delta = profile_memory(delta_stepping_sssp, graph,\n",
        "                               [float('inf')] * graph.num_nodes, source_node, delta)\n",
        "    mem_dijkstra = profile_memory(dijkstra_sssp, graph, source_node)\n",
        "    logger.info(\n",
        "        f\"Memory profiling disabled - Peak values not available\")\n",
        "\n",
        "    # Thread scaling analysis\n",
        "    thread_counts = [1, 2, 4, 8]\n",
        "    delta_times = []\n",
        "    for threads in thread_counts:\n",
        "        logger.info(f\"Running Delta-Stepping with {threads} threads...\")\n",
        "        start_time = time.time()\n",
        "        delta_stepping_sssp(graph, [float('inf')]\n",
        "                            * graph.num_nodes, source_node, delta)\n",
        "        delta_times.append(time.time() - start_time)\n",
        "    plot_results(thread_counts, delta_times, \"Delta-Stepping Thread Scaling\")\n",
        "\n",
        "    # Update impact analysis\n",
        "    num_updates_list = [10, 100, 1000]\n",
        "    incremental_times = []\n",
        "    for num_updates in num_updates_list:\n",
        "        updates = [EdgeUpdate(\n",
        "            UpdateType.INSERT if random.random() < 0.5 else UpdateType.DELETE,\n",
        "            random.randint(0, graph.num_nodes - 1),\n",
        "            random.randint(0, graph.num_nodes - 1)\n",
        "        ) for _ in range(num_updates)]\n",
        "\n",
        "        dist = [float('inf')] * graph.num_nodes\n",
        "        delta_stepping_sssp(graph, dist, source_node, delta)\n",
        "\n",
        "        start_time = time.time()\n",
        "        apply_updates(graph, updates)\n",
        "        affected_nodes = identify_affected_nodes(graph, updates, dist)\n",
        "        delta_stepping_sssp(graph, dist, source_node, delta, affected_nodes)\n",
        "        incremental_times.append(time.time() - start_time)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(num_updates_list, incremental_times,\n",
        "             marker='o', linestyle='-', color='r')\n",
        "    plt.xlabel(\"Number of Updates\")\n",
        "    plt.ylabel(\"Incremental SSSP Time (s)\")\n",
        "    plt.title(\"Impact of Updates on Performance\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"Update_Impact.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ipahHjj_UI3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}